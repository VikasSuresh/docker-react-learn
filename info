docker-cli uses docker server to exec command
docker uses a common os wheras heroku deployments creates a vm for a application
image - app with its config and dependencies
container - an instance of the image
the app sends command to kernel from kernel space/network/other hardwares are allocated
It creates sepereate segments in hard disk for an app this is done by either namespacing or controlgroups 
This entire docker run on a linux virtual machine cause the namespacing and controlgrps used to create.....
.... space in hdd is only available in linux .

docker run image_name  
docker ps && docker ps --all
docker create image_name cmd && docker start container_id
docker start container_id
docker system prune , docker rm container_id
docker logs container_id
docker stop or docker kill container_id
docker stop gives 10 sec to stop the process after 10 sec docker kill call is initiated
docker exec -it container_id redis-cli
docker run -it container sh
All the dockers have their own hdd. A container cannot access the files in another container

DockerFile - From Run cmd
Docker build . - in the same folder where dockerfile is created
A base image is something which has some predefined functions / commands so that it ease up the process
When the build starts the base image is fetched and it creates an temp container and perform run command....
....then it deletes the temp container then again when running it creates a temp container and deletes once its done...
....finally its stored in a main container
If the dockerfile hasnt changed then the build is stored in cache so if u build it again it builds faster.
Every container has an id to get a string instead of id we can use docker build -t vikassuresh/name
We can create an image from container using docker commit

Creating a docker file to host a node server using docker
use COPY to copy all files ... first copy package.json file
Build the image and then run it along with port
docker run -p 0010:8080 image_name here 0010 represents the system port and 8080 represents docker app port
its better to copy things into /usr/app use - WORKDIR /usr/app

Create a node project along with dockerfile 
For  connecting A node app with another redis or some other app we can use docker-compose
Docker compose is similar to docker-cli but instead of writing the code again and again this docker-compose..
...helps in creating a file for automating the process , it also allow u to create multiple container
services is  the place where u create containers
we dont have to code anything specific once u place the containers inside services it automatically allows ...
the containers to communicate we just have to pass the container name inside the code where we will be connecting

docker-compose up -  to start
docker-compose up - build rebuild
docker-compose up -d to run in background
docker-compose down to stop the process
restart: "no",always,on-failure, unless-stopped  to restart the container on condition basis 
docker-compose ps


Create a docker file for react-app u can run differnt docker file name using docker build -f file-name .

If u wanna new changes to reflect without rebuilding we can use volumes which instead of copying takes reference ....
of the folders. docker run -p 3000:3000 -v /app/node_modules -v $(pwd):/app container_id
Instead of using the above cmd everytime we can use docker-compose.yml to create volumes

it uses build: 
            context: .
            dockerfile: Dockerfile.dev
along with volumes:
            -   /app/node_modules
            - .:/app


In order to use production version of react (npm run build version) we have to use some server in order to route...
for that nginx is used

Push it to git
Sign-In in travis and install travis with git by using activate button in settings.
After that create a .travis.yml file and write down the docker commands make sure there is no unwanted indentation..
Push it to the git repo , travis automatically start a job for the pushed branch and check whether the tests...
written in the travis file is working ..

AWS -> login ->Elastic Bean stalk ->used for running one container
Create a An application and follow the process to create a app with docker as platform
the elastic bean stalk has a load balance which automatically scales up or down based on network traffic

for the app to get deployed when u push the branch u have to write a piece of code in travis.yml
need to add 
    deploy:
    provider:
    region:
    app:
    env:
    bucket_name:
    bucket_path
    on:
        branch:master // this makes sure that only master branch is deployed to elastic bean stalk
we have to create api keys which can be done from IAM in AWS 

and then save the api keys in travis env store 
use the key value created in travis env store in travis.yml
and create  access_key_id: $AWS_ACCESS_KEY
            secret_access_key: $AWS_SECRET_KEY
Expose Port in a dockerfile denotes that the app uses the corresponding port but in 
beanstalk it automatically routes the app via that port

In case if it throws any error use logs to find the errors .. it will be in eb-engine.log

Make sure We dont use as builder
Make sure we use COPY "package*.json ./"the star denotes copy both package.json and lock.json  and in aws specify ./ 

Create a sepereate dockerfile.dev for all the folder for the app with postgress
create docker-compose to access all postgress,redis and the api 
after creating volumes use environment: to add env variables
repeat the same process for all server client and worker
nginx decide which route should be directed to react-app and which one to express
the one denoted with /api is to make sure that it interacts with express app
create a nginx default.conf file.
dont name it as server in nginx cause its a reserved keyword
create a dockerfile.dev for nginx and mention it as service in docker-compose
We have to specify the pwd for postgress image and make sure that the nginx is wrapped....
under a http

During Deploying Mutliple Container it follows the below process:
Push -> Travis -> Travis Test -> Travis Prod ->Docker Hub -> EBS(Travis Pushes it to EBS) 
Create DockerFile for everything except client
In Client it uses two nginx one for routing and other for serving the react-scripts....
...We can put together into a single nginx server ... But the motive is to expose react port...
...so that the routing nginx can access react for that we have to write nginx.conf(which is inside client)...
This nginx is then copied to the docker using DockerFile
Create .travis.yml which creates run tests and push the prod version to docker Hub
For pushing create env variables in travis
And add - echo $DOCKER_PWD | docker login -u $DOCKER_ID --password-stdin in travis file
and add docker push for all the images , commit and push it to git.
after_install wont be working use after_success

Deploying it from DockerHub to AWS , in single container we didnt specify anything as aws itself will take ...
the dockerFile and understands it should create a container based on the docekerFile whereas in the present case...
it has multiple docker files so we have to specify which docker img to be used for that a dockerrun.aws.json file ...
should be created (EBS doesnt know to run a container, it uses elastic container service from aws to start container)...
for that the ec service uses task definition in order to create an container
Create a ebs application in aws
Security Group is kind of like firewall which tells which one can access which one cannot
Create a Security Group which can link all the three instance of EB, ElastiCache (Redis) ....
and RDS(Postgress)
Create ElastiCache and RDS
if u want to create an app in some other vpc we have to create a vpc and put all the ...
instance in that specific vpc
Create a security grp with the vpc and create inbound rules giving port-range and creating source...
as the current security group id
After Creating a Security Group we have to change the security grp for all the instance
Go to ElastiCache select the instance -> Modify -> Select the vpc along with the default one -> save ->modify
Go to RDS select the instance -> Modify -> add the vpc along with the default one  -> modify -> apply Immediately
Go to ebs -> configuration on right side -> instances -> select the vpc -> modify
After adding Security Group have to add env variables for that go to ebs instance and got to software ...
modify and then add env variables
After adding on pushing to github travis must push it to docker hub from there ebs should fetch it from docker hub...
for that we need to generate keys which can be done in IAM in aws
write the same deploy code we did it inthe docker-travis-ebs in travis
Make sure we place the secret key and access key inside travis
The code will throw an error because memory for each and every container should be mentioned
Check whether the required roles is there when u create a user in IAM
If that doesnt fix then go to ecs in aws -> cluster -> tasks -> stopped -> find whats wrong

Kubernetes
We will be using kubectl for issuing cmd
We have to create a deployment config which tells how to create and update instances of your application
Kubectl uses the Kubernetes API to interact with the cluster
kubectl get nodes and version
kubectl create deployment name linktorepo
kubectl get deployments
A Pod is a group of one or more containers, with shared storage and network resources...
and a specification for how to run the containers.
Every Kubernetes Node runs at least:
    Kubelet, a process responsible for communication between the Kubernetes Master and the Node; ...
        it manages the Pods and the containers running on a machine.
    A container runtime (like Docker) responsible for pulling the container image from a registry,...
         unpacking the container, and running the application.
kubectl get - list resources - kubectl get pods
kubectl describe - show detailed information about a resource - kubectl describe pods
kubectl logs - print the logs from a container in a pod - kubectl logs $POD_NAME
kubectl exec - execute a command on a container in a pod - kubectl exec $POD_NAME env
service is used to expose the node/container etc
label is like a nickname to the deployment
kubectl expose deployment/kubernetes-bootcamp --type="NodePort" --port 8080
kubectl delete service -l run=kubernetes-bootcamp
Scaling when kubectlg get deployments is used we get some fields:
    NAME lists the names of the Deployments in the cluster.
    READY shows the ratio of CURRENT/DESIRED replicas
    UP-TO-DATE displays the number of replicas that have been updated to achieve the desired state.
    AVAILABLE displays how many replicas of the application are available to your users.
    AGE displays the amount of time that the application has been running.
Here Desired is the no of replicas user defined whike creating a deployment and the current ...
is the one which is running
kubectl scale deployments/kubernetes-bootcamp --replicas=4
for scaling down use kubectl scale and change --replicas=2 eg: kubectl scale deployments/kubernetes-bootcamp --replicas=2
Upadting:
    kubectl set image deployments/kubernetes-bootcamp kubernetes-bootcamp=jocatalin/kubernetes-bootcamp:v2
    checking whether it has been updated:
        kubectl rollout status deployments/kubernetes-bootcamp


